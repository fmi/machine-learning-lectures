{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Класификационни и регресионни дървета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Днес: decision trees\n",
    "\n",
    "Преди това: да си припомним малко от предния път."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NumPy и векторизация\n",
    "\n",
    "Повечето операции в NumPy работят с така наречената векторизация. Това по-лесно се илюстрира с пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 4]) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.array([1, 2, 3]) * np.array([3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.array([8, 3, 4, 1, 9, 4]) > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(np.array([8, 3, 4, 1, 9, 4]) > 4).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.array([10, 20, 30])[np.array([1, 0, 2, 0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основната цел е да не ни се налага да пишем `for` цикли. Като допълнение – векторизираните операции в NumPy работят много по-бързо от цикъла в Python, който бихме написали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LabelEncoder\n",
    "\n",
    "Ако има категорийни данни (например низове), може да ползваме `LabelEncoder` да ги заменим с числа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit([\"red\", \"green\", \"red\", \"blue\", \"red\", \"green\"])\n",
    "\n",
    "colors = [\"green\", \"green\", \"blue\"]\n",
    "\n",
    "print(\"transofrmed:\", encoder.transform([\"green\", \"green\", \"blue\"])) \n",
    "print(\"inverse:    \", encoder.inverse_transform([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# OneHotEncoder\n",
    "\n",
    "Може да кодираме категории с label encoder, когато в категориите има някакъв естествен ред (напр. 4 е по-голямо от 2). Ако няма такъв ред обаче, на най-добре е да ползваме one-hot – така създаваме по един фийчър за всяка категория, който има стойности 0 или 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "encoder.fit([[0], \n",
    "             [1], \n",
    "             [0], \n",
    "             [2]])\n",
    "\n",
    "print(encoder.transform([[0], [1], [1], [2], [0]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Но да се върнем към днешната тема!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека генерираме синтетичен dataset подходящ за класификация. Може да погледнете [документацията](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) за значението на параметрите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100,\n",
    "                           n_features=2,\n",
    "                           n_redundant=0, \n",
    "                           n_clusters_per_class=2, \n",
    "                           random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` ще съдържа матрица от точки (всеки ред е точка с два координата), а `y` ще съдържа класа на всяка точка (0 или 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X[:4])\n",
    "print(y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека начертаем точките и техните класове в равнината:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това е една функция, която ще начертае decision boundary на decision tree (ще оцвети фона спрямо как класификатора ще определи дадена точка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting decision regions adapted from \n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
    "\n",
    "def plot_boundary(clf, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека тренираме класификатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier().fit(X, y)\n",
    "print(classifier.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Както виждаме, той успява да намери решение, което винаги класифицира правилно. Вероято прави голям overfitting. Да начертаем decision boundary-то:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От графиката изглежда че има overfitting – например има вертикалната ивица окола $x_0 = -1$ се класифицира изцяло като жълта, защото има една точка от този клас. По-вероятно е това да е някаква аномалия и да бихме искали да я сметнем за точка от другия клас.\n",
    "\n",
    "Бихме могли да регуляризираме с `min_samples_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_split=50).fit(X, y)\n",
    "plot_boundary(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тук получаваме по-прост модел, който естествено игнорира някои точки, но поне не overfit-ва.\n",
    "\n",
    "Нека начертаем дървото, което алгоритъма е открил:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# За да изпълните този код ще се нуждаете от graphviz библиотеката и съответния пакет за нея в python.\n",
    "#\n",
    "# На macOS това става със:\n",
    "#\n",
    "#  brew install graphviz\n",
    "#  pip install graphviz\n",
    "#\n",
    "# На други операционни системи трябва да замените първия ред с каквото ви е нужно (apt-get, yum, т.н.)\n",
    "\n",
    "def draw_decision_tree(classifier):\n",
    "    dot_data = tree.export_graphviz(classifier, out_file=None) \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    graph.render(\"tree\") \n",
    "\n",
    "    dot_data = tree.export_graphviz(classifier, out_file=None, \n",
    "                             feature_names=np.array(['X_0 Vertical', 'X_1 Horizontal']),  \n",
    "                             class_names=np.array(['Class_0', 'Class_1']),  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)\n",
    "\n",
    "    return graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "draw_decision_tree(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това е дървото на алгоритъма с `max_samples_split=50`. Може да видите няколко неща.\n",
    "\n",
    "Първо, алгоритъма се опитва да намери дърво от `if`-ове с които да отговори на класификационния въпрос. Първоначално сравнява $x_0$ (първия feature в `X` с $-0.5159$). Ако е по-малко, отговаря с оранжевия клас. Ако е по-голямо, сравнява $x_1$ с $-1.6008$ и на базата на резултата отговаря взема решение. Това съответства на правоъгълния в опростения алгоритъм.\n",
    "\n",
    "Във `samples` може да видите колко точки остават след вземането на решение. Например, първия възел ($x_0 < 0.5159$) ще раздели 100-те точки на два групи, една с 42 и друга с 58. Във `value` може да видите по колко точки има от всеки клас. Например, в лявото разклонение след корена на дървото попадаме в ситуация, в която в train set-а има 39 оранжеви и 3 сини и даваме отговор оранжеви (`Class_0`). Алгоритъма би могъл да продължи да разделя дървото, но понеже `samples = 42` е по-малко от `min_samples_split = 50` той спира да строи.\n",
    "\n",
    "`gini` е параметър, който помага на алгоритъма да прецени какво е най-доброто разпределение да дървото. Ще го разгледаме по-долу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека да видим как ще се справи алгоритъма без регуляризация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Така изглежда decision boundary-то:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Така изглежда неговото дърво:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Може да видите, че дървото е доста по-сложно, но за сметка на това отгатва train set-а перфектно. Ако проследите стойностите, може да се уверите, че съответства на графиката по-горе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# В `sklearn` е имплементиран CART алгоритъм. \n",
    "\n",
    "## Classification and Regression Trees\n",
    "\n",
    "L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Моделът CART е двоично дърво. \n",
    "\n",
    "* Всеки елемент от дървото може да има нула, едно или две деца.\n",
    "* Алгоритъмът работи рекурсивно. \n",
    "* Критерии за спиране могат да бъдат:\n",
    "  * Достигната е абсолютна чистота на елементите в дървото (останали са само един клас данни в последните деца)\n",
    "  * Достигната е максимална дълбочина на дървото (`max_depth`)\n",
    "  * Достигнат е минимален брой примери за разделяне (`min_samples_split`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Алгоритъмът е алчен (greedy):\n",
    "* Проверява всяка възможна колона (feature) за всички възможни раздвоявания и избира най-доброто.\n",
    "* Сложност при трениране: $O(n_{features}n_{samples}\\log(n_{samples}))$\n",
    "\n",
    "Следва малко математика, която не е съществена за разбиране на това какво се случва, но описва картинката една идея по-добре:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Оценяваща функция:\n",
    "\n",
    "Оценяващата функция работи, чрез Information Gain:\n",
    "\n",
    "$$ InformationGain = impurity(parent) - {WeightedAverageBySamplesCount}\\sum{impurity(children)}$$\n",
    "\n",
    "$impurity$ е функция, която измерва \"примесите\". В алгоритъма могат да се ползват различни функции:\n",
    "\n",
    "* При класификация\n",
    "  1. Ентропия (Entropy)\n",
    "  2. Gini\n",
    "  3. Неправилна класификация (Misclassification)\n",
    "* При регресия\n",
    "  1. Средно аритметично от разликата на квадратите (Mean Squared Error)\n",
    "  2. Средно аритметично от абсолютната стойност на разликата (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "При предвиждане на нов запис, алгоритъмът се спуска по построеното дърво докато стигне възел без наследници.\n",
    "\n",
    "* Предвижда средната стойнoст от записите останали в последния елемент при регресия.\n",
    "* При класификация избира класа представен от мнозинството от записи останали в последния елемент.\n",
    "* Сложност при предвиждане: $O(\\log(n_{samples}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функции за измерване на примесите (impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Ентропия\n",
    "\n",
    " $entropy = \\sum_i{ - p_i  log_2(  p_i )}$\n",
    " \n",
    " $p_i= \\frac{size of class_i}{size of set}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 1:\n",
    "\n",
    "Имаме множество:\n",
    "\n",
    "```python\n",
    "['Лъчо', 'Лъчо', 'Стефан', 'Стефан']\n",
    "```\n",
    "\n",
    "Лъчо - 2, Стефан - 2\n",
    "\n",
    "Пропорции:\n",
    "\n",
    "Лъчо: $\\frac{2}{4}$, Стефан: $\\frac{2}{4}$\n",
    "\n",
    "Стойност за Лъчо: \n",
    "$$-\\frac{2}{4} * log_2(\\frac{2}{4})$$\n",
    "$$-0.5 * -1$$\n",
    "$$0.5$$\n",
    "\n",
    "Стефан има същата стойност $0.5$.\n",
    "\n",
    "Ентропията на множеството е $$entropy=0.5+0.5 = 1.0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 2:\n",
    "\n",
    "```python\n",
    "['Круши', 'Круши', 'Круши']\n",
    "```\n",
    "\n",
    "Круши - 3\n",
    "\n",
    "Пропорции:\n",
    "\n",
    "Круши $\\frac{3}{3} = 1$\n",
    "\n",
    "$$-1 * log_2(1)$$\n",
    "$$-1 * 0$$\n",
    "$$entropy=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ето функция, която измерва това:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(subset_counts):\n",
    "    subset_counts = np.array(subset_counts)\n",
    "    subset_counts_normalized = subset_counts / subset_counts.sum()\n",
    "    \n",
    "    entropy = sum([-subset_count * np.log2(subset_count + 0.000000000001) \n",
    "                   for subset_count in subset_counts_normalized])\n",
    "    \n",
    "    entropy = np.round(entropy, 4)\n",
    "    print('Entropy for', subset_counts, 'is', entropy)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обърнете внимание на `+ 0.000000000001` в логаритъма. Мислете за това като хак, с който са си спестим малко сметки за да избегнем смятана на логаритъм от 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0], [1, 0], [9, 1], [4, 4]]\n",
    "\n",
    "for sample in samples:\n",
    "    entropy(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При два класа ентропията ще е в множеството $\\big[0, 1\\big]$. Имаме $1$ когато двата класа са равномерно разпределени в множеството и $0$, когато множеството съдържа елементи само от единия вид.\n",
    "\n",
    "При три или повече класа, функцията няма горна граница:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0, 1], [6, 0, 0], [9, 1, 0], [5, 5, 0], [5, 5, 5]]\n",
    "\n",
    "for sample in samples:\n",
    "    entropy(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Gini\n",
    "\n",
    "$H(X_m) = \\sum_k p_{mk} (1 - p_{mk})$\n",
    "\n",
    "$p_{mk}$ отново е пропорцията на класа в множеството."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример:\n",
    "\n",
    "```python\n",
    "['Лъчо', 'Лъчо', 'Стефан', Стефан']\n",
    "```\n",
    "\n",
    "Лъчо - 2, Стефан - 2\n",
    "\n",
    "Пропорции:\n",
    "\n",
    "Лъчо: $\\frac{2}{4}$, Стефан: $\\frac{2}{4}$\n",
    "\n",
    "Стойност за Лъчо: \n",
    "$$\\frac{2}{4} * (1 - \\frac{2}{4})$$\n",
    "$$0.5 * 0.5$$\n",
    "$$0.25$$\n",
    "\n",
    "Стефан има същата стойност $0.25$.\n",
    "\n",
    "$$gini=0.25+0.25 = 0.5$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 2:\n",
    "\n",
    "['Круши', 'Круши', 'Круши']\n",
    "\n",
    "Круши - 3\n",
    "\n",
    "Пропорции:\n",
    "\n",
    "Круши $\\frac{3}{3} = 1$\n",
    "\n",
    "$$1 * (1 - 1)$$\n",
    "$$1 * 0$$\n",
    "$$gini=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gini_impurity(subset_counts):\n",
    "    subset_counts = np.array(subset_counts)\n",
    "    subset_counts_normalized = subset_counts / subset_counts.sum()\n",
    "    \n",
    "    impurity = sum([subset_count * (1 - subset_count) \n",
    "                    for subset_count in subset_counts_normalized])\n",
    "    \n",
    "    print('Gini impurity for', subset_counts, 'is', impurity)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0], [1, 0], [9, 1], [4, 4]]\n",
    "\n",
    "for sample in samples:\n",
    "    gini_impurity(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0, 1], [6, 0, 0], [9, 1, 0], [5, 5, 0], [5, 5, 5]]\n",
    "\n",
    "for sample in samples:\n",
    "    gini_impurity(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Неправилна класификация (Misclassification)\n",
    "\n",
    "$H(X_m) = 1 - \\max(p_{mk})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def missclassification_impurity(subset_counts):\n",
    "    subset_counts = np.array(subset_counts)\n",
    "    subset_counts_normalized = subset_counts / subset_counts.sum()\n",
    "    \n",
    "    impurity = 1 - max(subset_counts_normalized)\n",
    "    \n",
    "    print('Misclassification impurity for', subset_counts, 'is', impurity)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0], [1, 0], [9, 1], [4, 4]]\n",
    "\n",
    "for sample in samples:\n",
    "    missclassification_impurity(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = [[2, 0, 1], [6, 0, 0], [9, 1, 0], [5, 5, 0], [5, 5, 5]]\n",
    "\n",
    "for sample in samples:\n",
    "    missclassification_impurity(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когато конструирате дърво, може да подадете критерий коя функция да се ползва. Това е параметъра `criterion`. scikit-learn поддържа само `gini` и `entropy`. Повече информация може да намерите в документацията:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всяка стъпка алгоритъма се опитва да намери неравенство, което да оптимизира information gain-а. Той се получава по следната формула:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$ InformationGain = impurity(parent) - \\sum_k{p_{mk}}{impurity(children)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Може да видим различните резултати, които бихме имали при различните функции за оценка на примесите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def information_gain(subsets, parent_impurity=1, f=entropy):\n",
    "    total_count = sum(sum(i) for i in subsets)\n",
    "    print(\"Total count:\", total_count)\n",
    "    \n",
    "    subsets_impurity = sum((sum(subset) / total_count * f(subset) for subset in subsets))\n",
    "    gain = parent_impurity - subsets_impurity\n",
    "    print(\"Information gain:\", gain)\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "subsets = [[2, 1], [1]]\n",
    "for f in [entropy, gini_impurity, missclassification_impurity]:\n",
    "    information_gain(subsets, parent_impurity=1, f=f); \n",
    "    print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "subsets = [[2], [2]]\n",
    "for f in [entropy, gini_impurity, missclassification_impurity]:\n",
    "    information_gain(subsets, parent_impurity=1, f=f); \n",
    "    print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gini_impurity([48, 52])\n",
    "gini_impurity([9, 49])\n",
    "gini_impurity([39, 3])\n",
    "gini_impurity([0, 46]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До тук с математиката. Не е нужно да запомните всичко, но ако ви се налага да дебъгвате какво е открил алгоритъма, това може да ви помогне да си го обясните."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Плюсове на decision tree \n",
    "* Дървото е лесно за интерпретация.\n",
    "* Лесно се справя с ирелевантни feature-и (gain = 0).\n",
    "* Може да се справи с липсващи данни (макар и не за текущата имплементация в sklearn).\n",
    "* Компактно представяне на модела.\n",
    "* Бърз при предсказване (линеен на дълбочината на дървото).\n",
    "* Може да прави класификация с повече класове без допълнителни трикове.\n",
    "* Лесен за използване и дава добри резултати с малко експерименти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Минуси\n",
    "\n",
    "* Разделя атрибутите само по осите. \n",
    "* Алчен (greedy) - може да не открие най-доброто дърво.\n",
    "* Експоненциално нарастване на възможните дървета.\n",
    "* Овърфитва силно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да се справим с проблемите на decision tree обикновено ползваме нещо, наречено random forest. То работи по следния начин:\n",
    "\n",
    "* Генерира много на брой дървета (10, 100, 1000, т.н.).\n",
    "* Всяко дърво е генерирано с различни критерии от останалите (подмножество на данните, подмножество на feature-ите и т.н.).\n",
    "* За да се отговори на въпрос, задаваме го на всички дървета и вземаме най-популярния отговор.\n",
    "\n",
    "Ето някои от причините това да работи по-добре:\n",
    "\n",
    "* Индивидуалните дървета ще правя overfitting, но по различен начин. Осредненият резултат между тях би трябвало да бъде по-регуляризиран.\n",
    "* Ако има аномалии в данните, те ще участват в по-малък брой дървета, съответно по-малко дървета ще overfit-ват.\n",
    "* Вариацията в кои feature се ползват ще изследва различни начини да отговорим на въпроса и докато всеки ще прави грешки, по-малко вероятно е всички да правят едни и същи грешки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В scikit-learn ползваме random forest по следния начин:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=23).fit(X, y) # без натройка на параметрите\n",
    "plot_boundary(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(подали сме `random_state=23` за да получаваме детерминистичен отговор – ако промените параметъра ще бъде намерена друга гора, което ще прави генерирането на едни и същи графики в този notebook трудно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Параметри за RF:\n",
    "\n",
    "* `n_estimators` – брой дървета (10, 100, 1000)\n",
    "* `criterion` – за всички дървета (gini, entropy)\n",
    "* `max_features` – колко feature да се пробват при търсене на най-добро разделяне. По подразбиране е `sqrt(n_features)`, като са различни при всяко ново търсене.\n",
    "* `max_depth` – максимална дълбочина на дърветата.\n",
    "* `min_samples_split` – минимален брой семпли за да може да се раздели възела\n",
    "* `bootstrap` – втори параметър за случайност - random sampling with replacement. Тегли същия брой семпли като оригиналния сет, но може да изтегли един елемен няколко пъти (и да не изтели друг)\n",
    "* `n_jobs` – тренира по няколко дървета едновременно, но използва повече памет\n",
    "* `random_state` – за възпроизведими експерименти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ето и две произволни дървета, без фиксиран `random_state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classifer = RandomForestClassifier().fit(X, y)\n",
    "plot_boundary(classifer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classifer = RandomForestClassifier().fit(X, y)\n",
    "plot_boundary(classifer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Значимост на feature-ите в random forest\n",
    "\n",
    "Сходно на теглата на параметрите при линейни модели, random forest може да ви каже кои feature-а намира за най-значими (т.е. носят най-много информация).\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_001.png)\n",
    "\n",
    "Може да видите повече тук:\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Прочетете документацията:\n",
    "* http://scikit-learn.org/stable/modules/tree.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
