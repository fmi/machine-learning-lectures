{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейна и логистична регресия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Целите днес:\n",
    "\n",
    "* Линейна регресия\n",
    "* Логистична регресия\n",
    "* Overfitting и underfitting\n",
    "* Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Преди това, нека си припомним неща от миналия път."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Какво са $X$ и $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ и $y$ са входните данни на нашия machine learning алгоритъм. $X$ е матрица (таблица) от данни, където всеки ред е инстанция на данните, а всяка колона е различна тяхна характеристика. $y$ пък е отговорът на въпроса, който търсим (labels).\n",
    "\n",
    "Ако се опитваме да оценим цената на апартаменти в град, всеки ред в $X$ е апартамент (чиято цена знаем), а всяка колона е различна характеристика на апартамента (площ, оценка на квартала, близост до метро, престъпност наоколо и т.н.). $y$ съдържа цените, като първият ред на $X$ съотвества на цената на първия ред от $y$, вторият ред на $X$ съответства на втория ред от $y$ и т.н."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Какво са feature-и (характеристики)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборът от характеристики, които знаем за данните. Алгоритмите търсят статистическа зависимост между тях и търсения отговор (напр. връзка между площ и цена)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Каква е разликата между supervised и unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При supervised learning имаме данни с label-и (вектора $y$) и търсим зависимости между $X$ и $y$. При unsupervised learning нямаме label-и и се опитваме да намерим генерални характеристики на dataset-а."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Каква е разликата между регресия и класификация?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регресията търси отговор в непрекъснато пространство (цена на апартамент), докато класификацията търси отговор, който обикновено е една от две стойности (дали даден имейл е спам, или не).\n",
    "\n",
    "Бележка: класификацията може да работи с няколко категории, като има различни похвати за това (напр. one-vs-all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Малко код, който ни трябва:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"sklearn\", message=\"^Objective did not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейна регресия\n",
    "\n",
    "Линейната регресия е алгоритъм, който се опитва да намери линейна функция (в линейно пространство), която приближава входните данни най-точно. Пространството има толкова измерения, колкото feature-и има в набора от данни.\n",
    "\n",
    "Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "При функция на един аргумент (едномерно линейно пространсто), функцията има вида:\n",
    "\n",
    "$$y = ax + b$$\n",
    "\n",
    "И алгоритъмът търси $a$ и $b$, които да минимизират общата грешка за елементите от $X$ и $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "При тримерно пространство, формулата е:\n",
    "\n",
    "$$y = ax_1 + bx_2 + cx_3 + d$$\n",
    "\n",
    "Алгоритъмът търси $a$, $b$, $c$ и $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Разбира се, това се генерализира за n-мерно пространство:\n",
    "\n",
    "$$y = a_0 + a_1 x_1 + a_2 x_2 + \\ldots + a_n x_n$$\n",
    "\n",
    "Където търсим $a_0 \\ldots a_n$.\n",
    "\n",
    "Обърнете вниманние, че има $a_0$, но няма $x_0$ (или поне, $x_0$ = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следва малко по-формална дефиниция, която на този етап може да пропуснете. Споменаваме я, защото става въпрос на лекцията.\n",
    "\n",
    "В горния пример, говорим за грешка $E$, която линейна функция $f$ има спрямо входните данни. Тя може да се дефинира така:\n",
    "\n",
    "$$ E(a) = \\sum_{k=1}^{M} \\big( f(x_k) - y_k \\big)^2 $$\n",
    "\n",
    "Където:\n",
    "\n",
    "* $M$ е броят на инстанциите на нашите данни (т.е. редовете в $X$)\n",
    "* $a$ и $x_i$ са вектори, $y_i$ и $f(x)$ са реални числа\n",
    "* $x_i \\in \\mathbb{R}^n$ е $i$-тият ред от $X$\n",
    "* $f: \\mathbb{R}^n\\to\\mathbb{R}$ е линейна функция в $n$-мерно пространство\n",
    "* $a \\in \\mathbb{R}^{n+1}$ са коефицентите на функцията $f$\n",
    "* $y_k$ е очакваната стойност на функцията за $k$-тия ред на $X$\n",
    "\n",
    "Грешката представлява сумата от квадратите на разликата между очакваната (в $y$) и предположената (от $f$) стойност. Алгоритъмът се опитва да намери коефиценти $a$ за които $E(a)$ е минимум. В общия случай това обикновено е глобален минимум, но това са детайли, които ще разгледаме отново по-натам. За повече детайл може да разгледате [курсът на Andrew Ng](https://www.coursera.org/learn/machine-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейна регресия – пример\n",
    "\n",
    "Нека имаме следния dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_data = np.array([\n",
    "    [0, 1],\n",
    "    [2, 1.7],\n",
    "    [8, 3],\n",
    "    [9, 3.1],\n",
    "    [10, 3.8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Горното е NumPy масив, който не е в съвсем правилната форма, която търсим. Ще го сведем до нея по следния начин:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = linear_data[:, 0:1]\n",
    "y = linear_data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` представлява матрица от входни данни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` пък е вектор от очаквани резултати."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Макар и математически да няма разлика между вектор и матрица с една колона, NumPy не работи точно така. Може условно да наричаме едномерния масив \"вектор\", докато двумерния \"матрица\", дори когато има само една колона.\n",
    "\n",
    "В Python това се представя като списък от списъци, макар че NumPy го свежда до оптимално представяне, което объркващо нарича \"масив\". За NumPy ще говорим по-подробно в следващи лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бихме могли да начертаем точките в графика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.gca().set_xlim(-2, 16)\n",
    "plt.gca().set_ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Тези точки изглеждат горе-долу на една права. На око, нейната функция е $y = 0.25x + 1$.\n",
    "\n",
    "При $x = 5$ очакваме нещо около $y = (0.25)(5) + 1 = 2.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.scatter(np.array([5]), np.array([2.25]), color='orange')\n",
    "plt.gca().set_xlim(-2, 16)\n",
    "plt.gca().set_ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нека ползваме `scikit-learn` за да намерим линейна функция с минимална грешка спрямо входните данни:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Това тренира модел (`model`), който открива коефициентите на функция, която търсим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`coef_` съдържа коефициентите (в случая един), а `intercept_` съдържа константата. Това е достатъчно близко до функцията, която предположихме:\n",
    "\n",
    "$$y = 0.25x + 1$$\n",
    "\n",
    "Бихме могли да я начертаем върху точките и да се уверим (на око), че това е така:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interval = np.linspace(-2, 16)\n",
    "result = interval * model.coef_[0] + model.intercept_\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(interval, result)\n",
    "plt.gca().set_xlim(-2, 16)\n",
    "plt.gca().set_ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Може да направим предвиждане за някоя от точките (например 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това е доста близо до очакваната стойност от наивния ни анализ ($2.25$).\n",
    "\n",
    "Обърнете внимание, че `predict` взема масив от стойности, за които да направи предположение. Бихме могли да направим предвиждане за няколко точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[5], [8], [12]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Генерализация\n",
    "\n",
    "\"Генарализация\" наричаме способността на модела да прави вярни предвиждания върху нови данни. Ако един модел се справя добре с това, казваме че генерализира добре.\n",
    "\n",
    "`scikit-learn` ни дава механизъм да оценим генерализацията:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Резултата от `score` е реално число.\n",
    "\n",
    "При класификация, тази оценка би била процент от случаите, в които алгоритъма предвижда отговора правилно. Например, ако $X$ бяха имейли, а $y$ е категория дали конкретен имейл е спам, `score = 0` би значело, че винаги даваме грешен отговор, а `score = 1` – винаги правилен.\n",
    "\n",
    "При регресия е малко по-сложно, защото малки грешки в отговора не са проблем на практика - няма значение дали алгоритъма ще предвици цена на апартамент $200,000лв$ или $200,375лв$. Формулата е по-сложна и не е толкова интересна на този етап ([детайли в документацията на scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)). Най-добрият резултат би бил 1, но числото може и да е отрицателно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейна регресия за нелинейни функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бихме могли да предвидим и нелинеен dataset. Например, нека пробваме с експоненциална функция: $2^x$\n",
    "\n",
    "Имаме следния набор от данни:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "exp_data = np.array([[0, 1], [1, 2],  [2, 4], [3, 8], [4, 16], [5, 32]])\n",
    "\n",
    "X, y = exp_data[:, 0:1], exp_data[:, 1]\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.gca().set_xlim(-1, 6)\n",
    "plt.gca().set_ylim(-1, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Тази графика не е линейна. Въпреки това, ако обработим входните данни, прекарвайки ги през логаритмична функция, те ще приемат линейна форма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y)\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X, y_log)\n",
    "plt.gca().set_xlim(-1, 6)\n",
    "plt.gca().set_ylim(-1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Може да използваме линейна регресия върху новите данни, след което да обработим резултата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y_log)\n",
    "\n",
    "prediction = model.predict(np.array([[8], [16]]))\n",
    "np.exp(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обърнете внимание, че прекарваме резултата от предположението през експоненциална функция (за да обърнем логаритъма).\n",
    "\n",
    "Ето как ще изглежда тази функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = np.linspace(-2, 16)\n",
    "result = np.exp(interval * model.coef_[0] + model.intercept_)\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(interval, result)\n",
    "plt.gca().set_xlim(-1, 6)\n",
    "plt.gca().set_ylim(-1, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting и underfitting\n",
    "\n",
    "Тези две концепции се срещат постоянно:\n",
    "\n",
    "* Underfitting – моделът се справя лошо с тренировъчните данни и генерализира лошо.\n",
    "* Overfitting – моделът се справя много добре с тренировъчните данни, но генерализира лошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да видим overfitting, нека пробваме да намерим полином от осма степен за оригиналния dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "linear_data = np.array([[0, 1], [2, 1.7], [8, 3], [9, 3.1], [10, 3.8]])\n",
    "X, y = linear_data[:, 0:1], linear_data[:, 1]\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X[:, 0], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Първо ще обработим оригиналните данни, създавайки нови feature-и – $x^2$, $x^3, \\ldots, x^8$. Така входните данни ще бъдат вектори с 8 елемента, като всеки feature съответства на степен на оригиналната стойност."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = np.concatenate((X, X ** 2, X ** 3, X ** 4, X ** 5, X ** 6, X ** 7, X ** 8), axis=1)\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нека да пробваме да тренираме модел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Този модел има следните коефициенти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.intercept_, model.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ако начертаем полинома с тези компоненти, получаваме следното:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "interval = np.linspace(-2, 16, num=200)\n",
    "\n",
    "@np.vectorize\n",
    "def polynomial(x):\n",
    "    return sum(a * x**(b+1) for (b, a) in enumerate(model.coef_)) + model.intercept_\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(interval, polynomial(interval))\n",
    "plt.gca().set_xlim(-1, 12); plt.gca().set_ylim(0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Тук виждаме overfitting – намира се много по-сложен модел, който уцелва всички точки от dataset-а, но генерализира лошо.\n",
    "\n",
    "Това е интуитивно – бихме могли да прокараме полином от осма степен през 9 произволни точки (камо ли пет). За сметка на това не, можем да намерим линейна функция за 5 точки точки (освен в частния случай, когато лежат на една права). Алгоритъмът намира функция без грешка на входните данни, но генерализира зле – $f(5)$ вече е стойност, съвсем различна от $2.25$. Допълнително, поведението извън интервала е $(0, 10)$ се разминава сериозно от очакваното."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(polynomial(5), polynomial(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Един от проблемите тук, е че имаме много малък dataset. При повече данни щяхме да имаме по-малък overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Регуляризация\n",
    "\n",
    "Повече модели прилагат някаква форма на регуляризация. Концепцията изисква повече математика за обяснение, но накратко, регуляризацията принуждава модела да бъде по-прост. Какво значи това зависи от конкретния алгоритъм.\n",
    "\n",
    "В случая на линейната регресия, регуляризацията принуждава алгоритъма да минимизира коефициентите в линейното уравнение. Ниски коефициенти резултират в по-прост линеен модел.\n",
    "\n",
    "Ще видим два вида регуляризация от `scikit-learn` – [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge) и [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Тази функция е написана отделно за да се спести място в слайдовете. Ще се ползва по-долу\n",
    "def draw_regularization(algorithm, alpha=1):\n",
    "    model = algorithm(alpha=alpha, max_iter=100000)\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    interval = np.linspace(-2, 16, num=200)\n",
    "\n",
    "    @np.vectorize\n",
    "    def polynomial(x):\n",
    "        return sum(a * x**(b+1) for (b, a) in enumerate(model.coef_)) + model.intercept_\n",
    "\n",
    "    plt.close()\n",
    "    plt.scatter(X[:, 0], y)\n",
    "    plt.plot(interval, polynomial(interval))\n",
    "    plt.gca().set_xlim(-1, 12); plt.gca().set_ylim(0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ridge\n",
    "\n",
    "`Ridge` използва нещо, наречено L2 регуляризация. Детайлите са по-сложни математически и ще ги покрием по-натам. Ефектът, е че графиката е \"по-изгладена\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "draw_regularization(Ridge, alpha=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X_poly, y)\n",
    "\n",
    "ridge = Ridge(alpha=10000)\n",
    "ridge.fit(X_poly, y)\n",
    "\n",
    "[regression.coef_, ridge.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Може да пробваме и с `Lasso`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=10000)\n",
    "lasso.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "draw_regularization(Lasso, alpha=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lasso` ползва L1 регуляризация. Разликата отново е математическа (и детайлите са нерелавнтни за момента), но `Lasso` може да намали някои коефициенти до 0, за разлика от `Ridge`. Така биха елиминирани feature-и, които не носят информация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & test sets\n",
    "\n",
    "Обикновено разделяме dataset-а на две части – training и test. Използваме training set-а да тренираме модела и test set-а да оценим как генерализира. Ако и двете числа са ниски, това е симптом за underfitting. Ако оценката на тренировъчния е висока, но тази на тестовия е ниска, обикновено има overfitting.\n",
    "\n",
    "`scikit-learn` предоставя функция за разделяне на dataset-а."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека пробваме с един синтетичен dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$0.66$ е ниско (около 66% вярно) - вероятно този модел underfit-ва."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Да пробваме с апартаментите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Тук вече има overfitting. Това лесно може да се случи при многомерни пространства.\n",
    "\n",
    "Може да пробваме да решим този проблем с регуляризация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Променяйки `alpha` параметъра получаваме различни резултати:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=10).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Резултатът тук се влошава – вероятно регуляризацията е твърде голяма. Може да пробваме да я намалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Тук имаме малко по-добър резултат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нека пробваме с `Lasso`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))\n",
    "print(\"Features used:  {}/{}\".format(np.sum(model.coef_ != 0), np.shape(X_train)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Резултатът е много по-лош, но виждаме, че само 4 feature-а се ползват. Алгоритъмът смята другите за неинформативни.\n",
    "\n",
    "Бихме могли да подобрим резултата като намалим регуляризацията:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))\n",
    "print(\"Features used:  {}/{}\".format(np.sum(model.coef_ != 0), np.shape(X_train)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ако продължим да я намаляваме, обаче, ще стигнем до overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score: {:.2f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score:     {:.2f}\".format(model.score(X_test, y_test)))\n",
    "print(\"Features used:  {}/{}\".format(np.sum(model.coef_ != 0), np.shape(X_train)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Логистична регресия\n",
    "\n",
    "Въпреки името, това е алгоритъм за класификация. Същото е като линейна регресия, единствената разлика е, че резултатът се прекарва през сигмоид (sigmoid).\n",
    "\n",
    "Сигмоида е тази функция:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "interval = np.linspace(-10, 10, num=1000)\n",
    "\n",
    "plt.close()\n",
    "plt.plot(interval, sigmoid(interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тази функция е 0.5 в нулата и клони към 1 за $+\\infty$ и към 0 за $-\\infty$.\n",
    "\n",
    "Може да ползваме двете граници за два отделни класа при класификация. Например, ако предвиждаме дали даден имейл е спам, 0 може да значи \"не\" и 1 може да значи \"да\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Може да я ползваме да класифицираме любимия на всички dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "regression = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(regression.score(X_train, y_train)))\n",
    "print(\"Test set score:     {:.3f}\".format(regression.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "С регуляризация може да постигнем и по-добри резултати:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(regression.score(X_train, y_train)))\n",
    "print(\"Test set score:     {:.3f}\".format(regression.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Няма нищо сложно и магическо в логистичната регресия – единствената разлика е, че линейната функция минава през сигмоид."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Малка статистическа бележка:\n",
    "\n",
    "Горният алгоритъм дава верен отговор в 95% от ситуациите. Това е информативно, когато двата класа са приблизително равномерно представени. Но ако единият е малък процент от другия, имаме проблем.\n",
    "\n",
    "Нека да си представим алгоритъм, който предвижда дали даден човек има рядко заболяване, налично в само 1% от хората. Ако 99% от dataset-а са хора без заболяването и 1% го има, тогава е много лесно да направим алгоритъм с 99% точност, като винаги отговаряме с \"не\". В такива случаи е важно да вземем предвид по-редкия случай и да го разгледаме отделно – по-важен въпрос е колко хора със заболяване биват правилно идентифицирани. Може да има и значение, ако различният вид грешка има различна цена – неправилно класифициран \"болен\" може да резултира в излишни (но безобидни) изследвания, но неправилно класифициран \"здрав\" рискува да пропусне лечение.\n",
    "\n",
    "Има малко математика, коияо може да са ни е полезна в такива случаи, но ще я разгледаме по-натам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Въпроси\n",
    "\n",
    "* http://fmi.machine-learning.bg\n",
    "* fmi@machine-learning.bg"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
